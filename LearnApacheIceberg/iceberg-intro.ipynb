{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f59c401-8295-45c2-8c4a-b14795681824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Source - https://stackoverflow.com/a/69297835\n",
    "# Posted by jmPicaza\n",
    "# Retrieved 2026-01-17, License - CC BY-SA 4.0\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae572a6-4e33-4491-9fe5-1f1d535ec56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 11:01:52 WARN Utils: Your hostname, codebase.local resolves to a loopback address: 127.0.0.1; using 192.168.1.7 instead (on interface en0)\n",
      "26/01/18 11:01:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/codebase/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/codebase/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4763e84e-f0d0-41ef-a3b2-0062a8961b57;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 in central\n",
      ":: resolution report :: resolve 46ms :: artifacts dl 1ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4763e84e-f0d0-41ef-a3b2-0062a8961b57\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/2ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 11:01:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>iceberg-intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109ecc200>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('iceberg-intro')\n",
    "    \n",
    "    # Iceberg package (same as --packages)\n",
    "    .config(\n",
    "        \"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2\"\n",
    "    )\n",
    "    \n",
    "    # Catalog configuration\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/Users/codebase/Documents/codebase/Courses/LearnApacheIceberg/warehouse\")\n",
    "        \n",
    "    .getOrCreate()\n",
    "    )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2bcda-ef1a-4301-9332-b9739197597f",
   "metadata": {},
   "source": [
    "#### Create an Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d802c02-2d5f-492f-ae2c-8477d19e4d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table using Iceberg\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE local.db.iceberg_table (\n",
    "        id BIGINT,\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        ts TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES (\n",
    "      'write.metadata.metrics.default' = 'truncate(16)'\n",
    "    )\n",
    "    PARTITIONED BY (days(ts))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026b221-db96-47e9-aadb-fcb47196b328",
   "metadata": {},
   "source": [
    "#### Insert Data into iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258ba9ef-9d32-4cbc-bb31-c890576ca812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert rows using SQL\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO local.db.iceberg_table VALUES\n",
    "    (1, 'Alice', 30, CURRENT_TIMESTAMP()),\n",
    "    (2, 'Bob', 25, CURRENT_TIMESTAMP())\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38aa5ff-269f-4ee7-af0d-a121404529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert rows using PySpark DataFrame\n",
    "from datetime import datetime as dt\n",
    "\n",
    "data = [(3, \"Charlie\", 28, dt.now()),\n",
    "        (4, \"Diana\", 34, dt.now())]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"ts\"])\n",
    "\n",
    "df.writeTo(\"local.db.iceberg_table\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b7d02-5112-44da-a8ad-864ac1f82c9e",
   "metadata": {},
   "source": [
    "#### Querying the Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7a25b4-0ebf-4ef5-86d6-c56efa458a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|                  ts|\n",
      "+---+-------+---+--------------------+\n",
      "|  3|Charlie| 28|2026-01-17 14:40:...|\n",
      "|  4|  Diana| 34|2026-01-17 14:40:...|\n",
      "|  1|  Alice| 30|2026-01-17 14:40:...|\n",
      "+---+-------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the Iceberg table\n",
    "result = spark.sql(\"SELECT * FROM local.db.iceberg_table WHERE age > 25\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e646077-9057-42af-ad8b-217fa9a6c853",
   "metadata": {},
   "source": [
    "#### Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d76369-47c5-4081-b679-5f7805842d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-17 14:40:...|3929309049952964673|               NULL|               true|\n",
      "|2026-01-17 14:40:...|8166654969485390864|3929309049952964673|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM local.db.iceberg_table.history\n",
    "    \"\"\"\n",
    ")\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc1e527-d36b-4353-854c-8978a99aaa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+--------------------+\n",
      "| id| name|age|                  ts|\n",
      "+---+-----+---+--------------------+\n",
      "|  1|Alice| 30|2026-01-17 14:40:...|\n",
      "|  2|  Bob| 25|2026-01-17 14:40:...|\n",
      "+---+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use snapshot ID to time travel\n",
    "snapshot_id = history.collect()[0]['snapshot_id']  # Fetching the snapshot ID\n",
    "\n",
    "\n",
    "time_travel_df = spark.read \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", snapshot_id) \\\n",
    "    .load(\"local.db.iceberg_table\")\n",
    "\n",
    "time_travel_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "668f8219-c7c1-458a-8910-c17ad476988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8166654969485390864"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.collect()[1]['snapshot_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b702a62-af1f-4adf-86c6-750ddb38a714",
   "metadata": {},
   "source": [
    "#### Query by timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330b0483-64e6-47bb-9db5-bd89d8147ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp_in_ms = 1768638825.557978  # Example: Jan 17, 2026, in milliseconds\n",
    "\n",
    "# time_travel_df = spark.read \\\n",
    "#     .format(\"iceberg\") \\\n",
    "#     .option(\"as-of-timestamp\", ) \\\n",
    "#     .load(\"local.db.iceberg_table\")\n",
    "\n",
    "# time_travel_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ed2d7-943b-40dd-a049-3048edc8423b",
   "metadata": {},
   "source": [
    "#### Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8368a535-6613-4b05-b9e9-b06f12e38a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column to the table\n",
    "spark.sql(\"ALTER TABLE local.db.iceberg_table ADD COLUMN email STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ba2dcf-eb4d-4c0e-83e3-3fb701e25325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'age' column from the table\n",
    "spark.sql(\"ALTER TABLE local.db.iceberg_table DROP COLUMN age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363ec6a8-7f9b-4937-82ab-73aa42f69d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-17 14:40:...|3929309049952964673|               NULL|               true|\n",
      "|2026-01-17 14:40:...|8166654969485390864|3929309049952964673|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM local.db.iceberg_table.history\n",
    "    \"\"\"\n",
    ").show()\n",
    "\n",
    "# No change in history -> because the data is not yet added after ALTER Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34463d79-78e5-48f5-bf5d-e5e240c2f028",
   "metadata": {},
   "source": [
    "#### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3042fe5e-19cd-4128-b28d-52ca8da48f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a partitioned Iceberg table by day based on the timestamp 'ts' column\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE local.db.partitioned_table (\n",
    "        id BIGINT,\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        ts TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(ts))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fa454-9a20-4007-ba03-e08568cc3ca6",
   "metadata": {},
   "source": [
    "#### Table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e4e732-99b1-46ff-b2ca-19d26dbbe52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------+\n",
      "|      col_name|data_type|comment|\n",
      "+--------------+---------+-------+\n",
      "|            id|   bigint|   NULL|\n",
      "|          name|   string|   NULL|\n",
      "|            ts|timestamp|   NULL|\n",
      "|         email|   string|   NULL|\n",
      "|              |         |       |\n",
      "|# Partitioning|         |       |\n",
      "|        Part 0| days(ts)|       |\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table schema\n",
    "schema = spark.sql(\"DESCRIBE local.db.iceberg_table\")\n",
    "schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea67e540-27cb-4ddd-bede-27550419e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2026-01-17 14:40:...|3929309049952964673|               NULL|               true|\n",
      "|2026-01-17 14:40:...|8166654969485390864|3929309049952964673|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history\n",
    "history = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM local.db.iceberg_table.history\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "history.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b97fcb9e-6439-4eca-bc9d-fbaeb0778c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|max(upper_bounds[ts])|\n",
      "+---------------------+\n",
      "|NULL                 |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT MAX(upper_bounds['ts'])\n",
    "    FROM local.db.iceberg_table.files\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bf28d92-12ff-4aa5-8fa0-61d9785287a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|max_ts|\n",
      "+------+\n",
      "|NULL  |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT MAX(data_file.upper_bounds['ts']) AS max_ts\n",
    "    FROM local.db.iceberg_table.entries\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cda81d59-1211-4a19-a6b5-8418f6ee5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------+\n",
      "|key                            |value              |\n",
      "+-------------------------------+-------------------+\n",
      "|current-snapshot-id            |8166654969485390864|\n",
      "|format                         |iceberg/parquet    |\n",
      "|format-version                 |2                  |\n",
      "|write.metadata.metrics.default |truncate(16)       |\n",
      "|write.parquet.compression-codec|zstd               |\n",
      "+-------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SHOW TBLPROPERTIES local.db.iceberg_table\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a68016c5-2fa3-4b4d-9144-c9aa107e868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|max(upper_bounds[ts])|\n",
      "+---------------------+\n",
      "|                 NULL|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT MAX(upper_bounds['ts'])\n",
    "    FROM local.db.iceberg_table.files;\n",
    "    \"\"\"\n",
    ").show()\n",
    "# MAX(partition.ts_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e9ad044-5cfb-4d29-8c61-a0d0f21d8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_travel_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4309490-bfc6-4ce8-b3b0-798f3d7a7fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    ALTER TABLE local.db.iceberg_table SET TBLPROPERTIES (\n",
    "      'write.metadata.metrics.default' = 'none',\n",
    "      'write.metadata.metrics.ts' = 'full',\n",
    "      'write.metadata.metrics.age' = 'full'\n",
    "    );\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72d2b10d-42f2-4768-9bb4-059093385fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    INSERT OVERWRITE local.db.iceberg_table\n",
    "    SELECT * FROM local.db.iceberg_table;\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b1736cb-148b-40c6-b94b-e9a6c6923b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|upper_bounds|lower_bounds|\n",
      "+------------+------------+\n",
      "|          {}|          {}|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "      upper_bounds,\n",
    "      lower_bounds\n",
    "    FROM local.db.iceberg_table.files\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    ").show()\n",
    "# MAX(partition.ts_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa2843-5fa6-4f4b-b2d2-c30cbda78b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
