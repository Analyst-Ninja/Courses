{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a8a0c82-d41f-44fd-aaa2-54e7e3572a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60b13bc5-1224-4870-852d-3d9cca25bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StringType,IntegerType, StructField, StructType\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('Axis-pyspark-refresher')\\\n",
    "    .config('spark.sql.seesion.logLevel','WARN')\\\n",
    "    .config('spark.executor.instances', '2')\\\n",
    "    .config('spark.executor.cores', '4')\\\n",
    "    .config('spark.executor.memory', '4g')\\\n",
    "    .config('spark.driver.memory','2g')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c707e5d5-bd24-4702-8928-e3f11c32fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_schema = StructType(\n",
    "    [\n",
    "        StructField(name='emp_id',dataType=IntegerType(),nullable=False),\n",
    "        StructField(name='name', dataType=StringType(), nullable=False),\n",
    "        StructField(name='age', dataType=IntegerType(), nullable=False),\n",
    "        StructField(name='dept_id', dataType=IntegerType(), nullable=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "emp_data = [\n",
    "    (1, \"John Doe\", 28, 101),\n",
    "    (2, \"Jane Smith\", 34, 102),\n",
    "    (3, \"Michael Johnson\", 45, 103),\n",
    "    (4, \"Emily Davis\", 25, 101),\n",
    "    (5, \"Sarah Wilson\", 29, None)\n",
    "]\n",
    "\n",
    "dept_schema = StructType(\n",
    "    [\n",
    "        StructField('dept_id',IntegerType(),False),\n",
    "        StructField('dept_name',StringType(),False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dept_data = [\n",
    "    (101, \"Sales\"),\n",
    "    (102, \"Marketing\"),\n",
    "    (103, \"Finance\"),\n",
    "    (104, \"HR\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "043152db-1595-4a37-aaac-6570ed856ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "dept_df = spark.createDataFrame(data=dept_data, schema=dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba94db39-42eb-4ea6-a38a-3e3913886bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+---+-------+-------+---------+\n",
      "|emp_id|           name|age|dept_id|dept_id|dept_name|\n",
      "+------+---------------+---+-------+-------+---------+\n",
      "|     1|       John Doe| 28|    101|    101|    Sales|\n",
      "|     4|    Emily Davis| 25|    101|    101|    Sales|\n",
      "|     2|     Jane Smith| 34|    102|    102|Marketing|\n",
      "|     3|Michael Johnson| 45|    103|    103|  Finance|\n",
      "+------+---------------+---+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.dept_id == dept_df.dept_id, 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bebb5136-4fec-428b-ae55-8d079a8668ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ccf7a69-f551-4a6d-a347-ba9f058e3c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+---+-------+\n",
      "|emp_id|           name|age|dept_id|\n",
      "+------+---------------+---+-------+\n",
      "|     1|       John Doe| 28|    101|\n",
      "|     2|     Jane Smith| 34|    102|\n",
      "|     3|Michael Johnson| 45|    103|\n",
      "|     4|    Emily Davis| 25|    101|\n",
      "|     5|   Sarah Wilson| 29|   NULL|\n",
      "|     1|       John Doe| 28|   NULL|\n",
      "|     2|     Jane Smith| 34|   NULL|\n",
      "|     3|Michael Johnson| 45|   NULL|\n",
      "|     4|    Emily Davis| 25|   NULL|\n",
      "|     5|   Sarah Wilson| 29|   NULL|\n",
      "+------+---------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.union(emp_df.select('emp_id','name','age',lit(None).alias('dept_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01e14c4d-4d67-4710-b5c8-4d16cd5c21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df_repartitioned = emp_df.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d34cd8d9-6455-4c3a-a435-60efc2ba958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/02 20:49:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+-----------------+------------------+\n",
      "|summary|            emp_id|        name|              age|           dept_id|\n",
      "+-------+------------------+------------+-----------------+------------------+\n",
      "|  count|                 5|           5|                5|                 4|\n",
      "|   mean|               3.0|        NULL|             32.2|            101.75|\n",
      "| stddev|1.5811388300841898|        NULL|7.854934754662192|0.9574271077563339|\n",
      "|    min|                 1| Emily Davis|               25|               101|\n",
      "|    max|                 5|Sarah Wilson|               45|               103|\n",
      "+-------+------------------+------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df_repartitioned.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf3dab95-a6a9-4493-bca3-d33aea2e5138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df_repartitioned.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44638eec-4e34-4295-aa6b-626d4e027cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
