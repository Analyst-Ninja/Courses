{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597bc3f9-c395-4c67-a59d-bd091d0c4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5def180-0a19-44ac-a935-dcfe24ef2478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "facaafab-2d04-46ad-9c57-8ebd496e10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8be9c5b0-c482-4f5c-8222-10dd3a4046f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State\n",
    "class BlogState(TypedDict):\n",
    "\n",
    "    topic: str\n",
    "    outline: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0af1b19a-105c-4e4b-9fbb-005b8613dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_outline(state: BlogState) -> BlogState:\n",
    "\n",
    "    # Extract topic\n",
    "    topic = state['topic']\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"Generate a outline for the topic - {topic}\"\n",
    "\n",
    "    # Generate outline\n",
    "    outline = model.invoke(prompt)\n",
    "\n",
    "    # Update State\n",
    "    state['outline'] = outline\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc7b269-abd3-45ca-b0fd-4b2f73aa06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blog(state: BlogState) -> BlogState:\n",
    "\n",
    "    # Extract topic and outline\n",
    "    topic = state['topic']\n",
    "    outline = state['outline']\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = f\"Generate a blog for the topic - {topic} with the outline given below\\n{outline}\"\n",
    "\n",
    "    # Generate blog\n",
    "    content = model.invoke(prompt)\n",
    "\n",
    "    # Update State\n",
    "    state['content'] = content\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c26551fb-204f-4e92-b210-9608acaebdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x77ca58f707a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define graph\n",
    "graph = StateGraph(BlogState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node('create_outline', create_outline)\n",
    "graph.add_node('create_blog', create_blog)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, 'create_outline')\n",
    "graph.add_edge('create_outline', 'create_blog')\n",
    "graph.add_edge('create_blog', END)\n",
    "\n",
    "# Compile workflow\n",
    "workflow = graph.compile()\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76d550a9-1765-4b2e-9abc-a4a2500f73b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='## The Transformer Architecture: Revolutionizing AI with Self-Attention\\n\\n**I. Introduction**\\n\\n*   **A. Hook:** The Transformer architecture has fundamentally reshaped the landscape of artificial intelligence, driving breakthroughs in fields ranging from natural language processing to computer vision with unprecedented accuracy and efficiency.\\n*   **B. Context:** Before Transformers, Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) were the dominant architectures for sequential data processing. However, they suffered from limitations such as difficulty in parallelization and capturing long-range dependencies.\\n*   **C. Thesis Statement:** The transformer architecture is a neural network architecture that relies on self-attention mechanisms to process sequential data in parallel, overcoming limitations of previous architectures and achieving state-of-the-art performance in various AI tasks.\\n*   **D. Outline Preview:** This blog post will delve into the intricacies of the Transformer architecture, exploring its building blocks, advantages, applications, variations, limitations, and future directions.\\n\\n**II. Background: Limitations of Previous Architectures**\\n\\n*   **A. Recurrent Neural Networks (RNNs):**\\n    *   1.  **Sequential Processing:** RNNs process data sequentially, making parallelization difficult and limiting their ability to leverage modern hardware.\\n    *   2.  **Vanishing/Exploding Gradients:** RNNs struggle to learn long-range dependencies due to the vanishing or exploding gradient problem, hindering their performance on long sequences.\\n    *   3.  **Difficulty in capturing long-range dependencies:** As the sequence length increases, RNNs often fail to capture the relationships between distant elements.\\n*   **B. Convolutional Neural Networks (CNNs):**\\n    *   1.  **Limited Contextual Understanding:** CNNs primarily focus on local features, making it challenging to capture the global context of a sequence.\\n    *   2.  **Requires deeper networks to capture long-range dependencies:** To capture long-range dependencies, CNNs require deeper networks, increasing computational complexity and potentially leading to overfitting.\\n\\n**III. The Transformer Architecture: Building Blocks**\\n\\n*   **A. Self-Attention Mechanism:**\\n    *   1.  **Explanation of the concept:** Self-attention allows the model to attend to different parts of the input sequence when processing each element, capturing relationships and dependencies between them.\\n    *   2.  **Queries, Keys, and Values:** In self-attention, each word in the input sequence is transformed into three vectors: a query, a key, and a value. The query represents the word being processed, while the keys and values represent all other words in the sequence.\\n    *   3.  **Scaled Dot-Product Attention:** The scaled dot-product attention mechanism calculates the attention weights by taking the dot product of the query and each key, scaling the result by the square root of the dimension of the keys, and applying a softmax function to obtain the attention weights. This can be represented as:\\n        `Attention(Q, K, V) = softmax((QK^T) / sqrt(d_k))V`\\n        where Q is the matrix of queries, K is the matrix of keys, V is the matrix of values, and d_k is the dimension of the keys. The scaling factor prevents the dot products from becoming too large, which can lead to small gradients and difficulty in training.\\n    *   4.  **Multi-Head Attention:** Multi-head attention allows the model to attend to different aspects of the input sequence by using multiple sets of queries, keys, and values. Each \"head\" learns a different attention pattern, capturing different types of relationships between words. The outputs of the different heads are then concatenated and linearly transformed to produce the final output.\\n*   **B. Encoder-Decoder Structure:**\\n    *   1.  **Encoder:** The encoder processes the input sequence and transforms it into a contextualized representation.\\n        *   a.  **Stack of Encoder Layers:** The encoder consists of a stack of identical layers. Each layer typically includes a self-attention sub-layer, a feed-forward network sub-layer, residual connections around each sub-layer, and layer normalization after each sub-layer. The self-attention sub-layer captures relationships between words in the input sequence, while the feed-forward network further processes the information. Residual connections and layer normalization help to train deeper networks and improve convergence.\\n    *   2.  **Decoder:** The decoder generates the output sequence based on the encoder\\'s output and previously generated tokens.\\n        *   a.  **Masked Self-Attention:** The decoder uses a masked self-attention mechanism to prevent it from attending to future tokens during training, ensuring that the model only uses information from the past to predict the next token.\\n        *   b.  **Encoder-Decoder Attention:** The decoder also uses an encoder-decoder attention mechanism to attend to the encoder\\'s output, allowing it to incorporate information from the input sequence when generating the output sequence.\\n        *   c.  **Stack of Decoder Layers:** Similar to the encoder, the decoder consists of a stack of identical layers. Each layer typically includes a masked self-attention sub-layer, an encoder-decoder attention sub-layer, a feed-forward network sub-layer, residual connections around each sub-layer, and layer normalization after each sub-layer.\\n*   **C. Positional Encoding:**\\n    *   1.  **Necessity:** Since Transformers do not have inherent sequential information (permutation-invariant), positional encoding is crucial for providing information about the position of tokens in the sequence.\\n    *   2.  **Different techniques:**\\n        *   **Sinusoidal positional encodings (original paper):** Uses sine and cosine functions of different frequencies to represent the position of each token.\\n        *   **Learned positional embeddings:** Learns a unique embedding for each position in the sequence.\\n*   **D. Residual Connections and Layer Normalization:**\\n    *   1.  **Importance for training deep networks:** Residual connections and layer normalization are essential for training deep Transformer networks, as they help to mitigate the vanishing gradient problem and improve convergence.\\n    *   2.  **Explanation of how they are used in the Transformer architecture:** Residual connections add the input of a sub-layer to its output, allowing the gradient to flow more easily through the network. Layer normalization normalizes the activations within each layer, stabilizing the training process.\\n\\n**IV. Advantages of Transformer Architecture**\\n\\n*   **A. Parallelization:** Transformers can process the entire input sequence in parallel, resulting in significant speed-up in training compared to RNNs.\\n*   **B. Long-Range Dependencies:** Self-attention allows Transformers to capture long-range dependencies more effectively than RNNs, as it can directly attend to any part of the input sequence.\\n*   **C. Contextual Understanding:** By attending to different parts of the input sequence, Transformers gain a better understanding of the context of words and phrases.\\n*   **D. Scalability:** Transformers can be scaled to handle large amounts of data, making them suitable for training on massive datasets.\\n\\n**V. Applications of Transformer Architecture**\\n\\n*   **A. Natural Language Processing (NLP):**\\n    *   1.  **Machine Translation (e.g., Google Translate):** Transformers have revolutionized machine translation, achieving state-of-the-art results in translating between different languages.\\n    *   2.  **Text Summarization:** Transformers can generate concise and informative summaries of long documents.\\n    *   3.  **Question Answering:** Transformers can answer questions based on a given context.\\n    *   4.  **Sentiment Analysis:** Transformers can determine the sentiment (positive, negative, or neutral) of a given text.\\n    *   5.  **Text Generation:** Transformers can generate realistic and coherent text, such as articles, stories, and poems.\\n*   **B. Computer Vision (CV):**\\n    *   1.  **Image Classification (e.g., Vision Transformer - ViT):** Transformers can classify images into different categories.\\n    *   2.  **Object Detection:** Transformers can identify and locate objects within an image.\\n    *   3.  **Image Segmentation:** Transformers can segment an image into different regions, assigning a label to each region.\\n    *   4.  **Image Generation:** Transformers can generate realistic and high-quality images.\\n*   **C. Speech Recognition:**\\n    *   1.  **End-to-end speech recognition systems:** Transformers have enabled the development of end-to-end speech recognition systems that directly transcribe speech into text.\\n*   **D. Other Applications:**\\n    *   1.  **Time Series Analysis:** Transformers can be used to analyze time series data, such as stock prices and weather patterns.\\n    *   2.  **Bioinformatics:** Transformers can be applied to various bioinformatics tasks, such as protein structure prediction and drug discovery.\\n\\n**VI. Variations and Extensions of Transformer Architecture**\\n\\n*   **A. BERT (Bidirectional Encoder Representations from Transformers):** BERT is a pre-training technique that uses a bidirectional Transformer encoder to learn contextualized word embeddings.\\n*   **B. GPT (Generative Pre-trained Transformer):** GPT is a pre-training technique that uses a Transformer decoder to generate text.\\n*   **C. T5 (Text-to-Text Transfer Transformer):** T5 reformulates all NLP tasks as text-to-text problems, allowing a single model to be used for multiple tasks.\\n*   **D. Other Architectures:** Numerous modifications and improvements have been made to the original Transformer architecture, leading to the development of new and more powerful models.\\n\\n**VII. Challenges and Limitations**\\n\\n*   **A. Computational Cost:** Training large Transformer models can be computationally expensive, requiring significant resources and time.\\n*   **B. Data Requirements:** Transformers typically require large amounts of data for training to achieve optimal performance.\\n*   **C. Interpretability:** It can be difficult to interpret the decisions made by Transformer models, making it challenging to understand why they make certain predictions.\\n*   **D. Ethical Considerations:** Transformer models can be biased if trained on biased data, leading to unfair or discriminatory outputs.\\n\\n**VIII. Future Directions**\\n\\n*   **A. Reducing Computational Cost:** Research is ongoing to develop more efficient Transformer architectures that require less computational resources.\\n*   **B. Improving Interpretability:** Researchers are exploring methods for understanding how Transformers make decisions, such as attention visualization and probing techniques.\\n*   **C. Addressing Ethical Concerns:** Efforts are being made to develop methods for mitigating bias in Transformer models, such as data augmentation and adversarial training.\\n*   **D. Exploring New Applications:** Researchers are continuously exploring new applications of Transformers in various fields, pushing the boundaries of what is possible with AI.\\n\\n**IX. Conclusion**\\n\\n*   **A. Restate Thesis:** The Transformer architecture has revolutionized artificial intelligence, enabling breakthroughs in various fields with its parallel processing capabilities and self-attention mechanism.\\n*   **B. Recap Key Points:** The key advantages of Transformers include parallelization, capturing long-range dependencies, contextual understanding, and scalability. They have found widespread applications in NLP, computer vision, and other domains.\\n*   **C. Future Outlook:** The Transformer architecture will continue to play a crucial role in AI research and development, driving innovation and enabling new applications.\\n*   **D. Closing Statement:** The transformative power of Transformers in the field of artificial intelligence is undeniable, and their continued evolution promises to unlock even greater potential in the years to come.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--88f25fce-f6bf-48a9-9672-6eac2318e9c4-0' usage_metadata={'input_tokens': 1575, 'output_tokens': 2436, 'total_tokens': 4011, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Define initial State\n",
    "topic = \"What is transformer architecture in AI?\"\n",
    "\n",
    "initial_state = {\n",
    "    'topic': topic,\n",
    "    'outline': \"\",\n",
    "    'content': \"\"\n",
    "}\n",
    "\n",
    "# Execute workflow\n",
    "final_state = workflow.invoke(initial_state)\n",
    "\n",
    "print(final_state['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
