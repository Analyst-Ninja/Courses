{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fac8aa1-c600-4e46-8523-db6f82290302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langsmith import traceable\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be08958-282f-4b61-a15d-fb164758633e",
   "metadata": {},
   "source": [
    "##### Basic Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d12ba-fe0e-417c-933f-ed94de5f8352",
   "metadata": {},
   "source": [
    "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is _very_ use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
    "\n",
    "* **Rules for our LLM**: this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the _system prompt_ of an chat LLM.\n",
    "\n",
    "* **Context**: this part is RAG-specific. The context refers to some _external information_ that we may have retrieved from a web search, database query, or often a _vector database_. This external information is the **R**etrieval **A**ugmentation part of **RA**G. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
    "\n",
    "* **Question**: this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a _user message_). However, the format and location of this being provided often changes.\n",
    "\n",
    "* **Answer**: this is the answer from our assistant, again this is _very_ typical and we'd expect this with every use-case.\n",
    "\n",
    "The below is an example of how a RAG prompt may look:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26364a02-0ee5-4247-b886-de92dd56ad64",
   "metadata": {},
   "source": [
    "```\n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25300b-89c2-436f-bcb9-fcf7673dab14",
   "metadata": {},
   "source": [
    "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076bf87e-e7d0-41b9-b159-adfc5afd1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888e74e-b0fb-4555-b9fa-99cde51ea39e",
   "metadata": {},
   "source": [
    "LangChain uses a `ChatPromptTemplate` object to format the various prompt types into a single list which will be passed to our LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befa9804-4ab1-4497-a710-a6b5ba178252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# passing the template to the LangChain model\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9302d8-e5eb-4bf2-ac3b-ec81e9771f87",
   "metadata": {},
   "source": [
    "When we call the template it will expect us to provide two variables, the `context` and the `query`. Both of these variables are pulled from the strings we wrote, as LangChain interprets curly-bracket syntax (ie `{context}` and `{query}`) as indicating a dynamic variable that we expect to be inserted at query time. We can see that these variables have been picked up by our template object by viewing it's `input_variables` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e585d7-5de6-422e-9691-7bf1f1e740f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64cb358-be20-43e0-9252-5df7a541919e",
   "metadata": {},
   "source": [
    "We can also view the structure of the messages (currently _prompt templates_) that the `ChatPromptTemplate` will construct by viewing the `messages` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff3025c-0c8e-416b-a723-23cddd9a2d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a94785-14cf-412a-89e7-3d19ebadd13b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
