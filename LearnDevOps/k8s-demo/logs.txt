
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMMAND   ‚îÇ              ARGS               ‚îÇ PROFILE  ‚îÇ USER ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start      ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 13:20 IST ‚îÇ 21 Sep 25 13:22 IST ‚îÇ
‚îÇ stop       ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 14:29 IST ‚îÇ 21 Sep 25 14:29 IST ‚îÇ
‚îÇ start      ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:24 IST ‚îÇ 21 Sep 25 17:24 IST ‚îÇ
‚îÇ stop       ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:29 IST ‚îÇ 21 Sep 25 17:29 IST ‚îÇ
‚îÇ start      ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:31 IST ‚îÇ 21 Sep 25 17:31 IST ‚îÇ
‚îÇ image      ‚îÇ load kubernetes-demo-api:latest ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:32 IST ‚îÇ 21 Sep 25 17:32 IST ‚îÇ
‚îÇ docker-env ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:32 IST ‚îÇ 21 Sep 25 17:32 IST ‚îÇ
‚îÇ start      ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:38 IST ‚îÇ                     ‚îÇ
‚îÇ stop       ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:39 IST ‚îÇ 21 Sep 25 17:39 IST ‚îÇ
‚îÇ start      ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:39 IST ‚îÇ 21 Sep 25 17:39 IST ‚îÇ
‚îÇ service    ‚îÇ devops-kubernetes-api-service   ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:41 IST ‚îÇ                     ‚îÇ
‚îÇ service    ‚îÇ devops-kubernetes-api-service   ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:42 IST ‚îÇ 21 Sep 25 17:44 IST ‚îÇ
‚îÇ stop       ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 21 Sep 25 17:45 IST ‚îÇ 21 Sep 25 17:45 IST ‚îÇ
‚îÇ start      ‚îÇ                                 ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 22 Sep 25 20:48 IST ‚îÇ 22 Sep 25 20:48 IST ‚îÇ
‚îÇ service    ‚îÇ devops-kubernetes-api-service   ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 22 Sep 25 20:52 IST ‚îÇ                     ‚îÇ
‚îÇ service    ‚îÇ devops-kubernetes-api-service   ‚îÇ minikube ‚îÇ eren ‚îÇ v1.37.0 ‚îÇ 22 Sep 25 20:53 IST ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/09/22 20:48:42
Running on machine: codebase
Binary: Built with gc go1.24.6 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0922 20:48:42.042512   69206 out.go:360] Setting OutFile to fd 1 ...
I0922 20:48:42.042903   69206 out.go:413] isatty.IsTerminal(1) = true
I0922 20:48:42.042904   69206 out.go:374] Setting ErrFile to fd 2...
I0922 20:48:42.042906   69206 out.go:413] isatty.IsTerminal(2) = true
I0922 20:48:42.043647   69206 root.go:338] Updating PATH: /Users/codebase/.minikube/bin
W0922 20:48:42.043788   69206 root.go:314] Error reading config file at /Users/codebase/.minikube/config/config.json: open /Users/codebase/.minikube/config/config.json: no such file or directory
I0922 20:48:42.045739   69206 out.go:368] Setting JSON to false
I0922 20:48:42.062352   69206 start.go:130] hostinfo: {"hostname":"codebase.local","uptime":114438,"bootTime":1758439884,"procs":822,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"26.0","kernelVersion":"25.0.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"a61e5cb6-e0ed-5033-bab6-334f5acdad82"}
W0922 20:48:42.063096   69206 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0922 20:48:42.067958   69206 out.go:179] üòÑ  minikube v1.37.0 on Darwin 26.0 (arm64)
I0922 20:48:42.075566   69206 notify.go:220] Checking for updates...
I0922 20:48:42.075733   69206 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0922 20:48:42.076723   69206 driver.go:421] Setting default libvirt URI to qemu:///system
I0922 20:48:42.098928   69206 docker.go:123] docker version: linux-28.3.3:Docker Desktop 4.45.0 (203075)
I0922 20:48:42.099140   69206 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0922 20:48:42.160326   69206 info.go:266] docker info: {ID:9dce8e22-cfac-491b-bbf7-b20c253a357e Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:26 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:57 OomKillDisable:false NGoroutines:102 SystemTime:2025-09-22 15:18:42.150764847 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8218034176 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/codebase/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.3.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/codebase/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/Users/codebase/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.27.0-desktop.1] map[Name:cloud Path:/Users/codebase/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.21] map[Name:compose Path:/Users/codebase/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.2-desktop.1] map[Name:debug Path:/Users/codebase/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:/Users/codebase/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/Users/codebase/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/Users/codebase/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/codebase/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.16.0] map[Name:model Path:/Users/codebase/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.39] map[Name:sbom Path:/Users/codebase/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/codebase/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0922 20:48:42.164981   69206 out.go:179] ‚ú®  Using the docker driver based on existing profile
I0922 20:48:42.172942   69206 start.go:304] selected driver: docker
I0922 20:48:42.172957   69206 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0922 20:48:42.172988   69206 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0922 20:48:42.173090   69206 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0922 20:48:42.247200   69206 info.go:266] docker info: {ID:9dce8e22-cfac-491b-bbf7-b20c253a357e Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:26 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:57 OomKillDisable:false NGoroutines:102 SystemTime:2025-09-22 15:18:42.238472472 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8218034176 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/codebase/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.3.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/codebase/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/Users/codebase/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.27.0-desktop.1] map[Name:cloud Path:/Users/codebase/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.21] map[Name:compose Path:/Users/codebase/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.2-desktop.1] map[Name:debug Path:/Users/codebase/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:/Users/codebase/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/Users/codebase/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/Users/codebase/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/codebase/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.16.0] map[Name:model Path:/Users/codebase/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.39] map[Name:sbom Path:/Users/codebase/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/codebase/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0922 20:48:42.247383   69206 cni.go:84] Creating CNI manager for ""
I0922 20:48:42.247993   69206 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0922 20:48:42.248194   69206 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0922 20:48:42.255921   69206 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0922 20:48:42.259999   69206 cache.go:123] Beginning downloading kic base image for docker with docker
I0922 20:48:42.263762   69206 out.go:179] üöú  Pulling base image v0.0.48 ...
I0922 20:48:42.270063   69206 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0922 20:48:42.270063   69206 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I0922 20:48:42.270106   69206 preload.go:146] Found local preload: /Users/codebase/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-arm64.tar.lz4
I0922 20:48:42.270109   69206 cache.go:58] Caching tarball of preloaded images
I0922 20:48:42.270324   69206 preload.go:172] Found /Users/codebase/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0922 20:48:42.270334   69206 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0922 20:48:42.270401   69206 profile.go:143] Saving config to /Users/codebase/.minikube/profiles/minikube/config.json ...
I0922 20:48:42.394403   69206 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I0922 20:48:42.395349   69206 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I0922 20:48:42.395375   69206 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I0922 20:48:42.395386   69206 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I0922 20:48:42.395390   69206 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I0922 20:48:42.395392   69206 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I0922 20:48:46.274732   69206 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I0922 20:48:46.274848   69206 cache.go:232] Successfully downloaded all kic artifacts
I0922 20:48:46.274938   69206 start.go:360] acquireMachinesLock for minikube: {Name:mka594d8675de76386ccc961e30f2aab1a668c1d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0922 20:48:46.275271   69206 start.go:364] duration metric: took 281.417¬µs to acquireMachinesLock for "minikube"
I0922 20:48:46.275798   69206 start.go:96] Skipping create...Using existing machine configuration
I0922 20:48:46.275805   69206 fix.go:54] fixHost starting: 
I0922 20:48:46.276268   69206 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0922 20:48:46.295957   69206 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0922 20:48:46.295986   69206 fix.go:138] unexpected machine state, will restart: <nil>
I0922 20:48:46.300061   69206 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I0922 20:48:46.300446   69206 cli_runner.go:164] Run: docker start minikube
I0922 20:48:46.424374   69206 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0922 20:48:46.443908   69206 kic.go:430] container "minikube" state is running.
I0922 20:48:46.444853   69206 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0922 20:48:46.456426   69206 profile.go:143] Saving config to /Users/codebase/.minikube/profiles/minikube/config.json ...
I0922 20:48:46.456709   69206 machine.go:93] provisionDockerMachine start ...
I0922 20:48:46.456914   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:46.468384   69206 main.go:141] libmachine: Using SSH client type: native
I0922 20:48:46.468910   69206 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c37c0] 0x1050c5f80 <nil>  [] 0s} 127.0.0.1 55012 <nil> <nil>}
I0922 20:48:46.468914   69206 main.go:141] libmachine: About to run SSH command:
hostname
I0922 20:48:46.469529   69206 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0922 20:48:49.616511   69206 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0922 20:48:49.616553   69206 ubuntu.go:182] provisioning hostname "minikube"
I0922 20:48:49.616803   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:49.648692   69206 main.go:141] libmachine: Using SSH client type: native
I0922 20:48:49.648997   69206 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c37c0] 0x1050c5f80 <nil>  [] 0s} 127.0.0.1 55012 <nil> <nil>}
I0922 20:48:49.649007   69206 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0922 20:48:49.809304   69206 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0922 20:48:49.809513   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:49.839707   69206 main.go:141] libmachine: Using SSH client type: native
I0922 20:48:49.840006   69206 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c37c0] 0x1050c5f80 <nil>  [] 0s} 127.0.0.1 55012 <nil> <nil>}
I0922 20:48:49.840015   69206 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0922 20:48:49.975361   69206 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0922 20:48:49.975392   69206 ubuntu.go:188] set auth options {CertDir:/Users/codebase/.minikube CaCertPath:/Users/codebase/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/codebase/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/codebase/.minikube/machines/server.pem ServerKeyPath:/Users/codebase/.minikube/machines/server-key.pem ClientKeyPath:/Users/codebase/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/codebase/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/codebase/.minikube}
I0922 20:48:49.975444   69206 ubuntu.go:190] setting up certificates
I0922 20:48:49.975454   69206 provision.go:84] configureAuth start
I0922 20:48:49.975640   69206 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0922 20:48:50.007124   69206 provision.go:143] copyHostCerts
I0922 20:48:50.007713   69206 exec_runner.go:144] found /Users/codebase/.minikube/ca.pem, removing ...
I0922 20:48:50.007902   69206 exec_runner.go:203] rm: /Users/codebase/.minikube/ca.pem
I0922 20:48:50.008094   69206 exec_runner.go:151] cp: /Users/codebase/.minikube/certs/ca.pem --> /Users/codebase/.minikube/ca.pem (1074 bytes)
I0922 20:48:50.008625   69206 exec_runner.go:144] found /Users/codebase/.minikube/cert.pem, removing ...
I0922 20:48:50.008628   69206 exec_runner.go:203] rm: /Users/codebase/.minikube/cert.pem
I0922 20:48:50.008713   69206 exec_runner.go:151] cp: /Users/codebase/.minikube/certs/cert.pem --> /Users/codebase/.minikube/cert.pem (1115 bytes)
I0922 20:48:50.009095   69206 exec_runner.go:144] found /Users/codebase/.minikube/key.pem, removing ...
I0922 20:48:50.009101   69206 exec_runner.go:203] rm: /Users/codebase/.minikube/key.pem
I0922 20:48:50.009197   69206 exec_runner.go:151] cp: /Users/codebase/.minikube/certs/key.pem --> /Users/codebase/.minikube/key.pem (1679 bytes)
I0922 20:48:50.009499   69206 provision.go:117] generating server cert: /Users/codebase/.minikube/machines/server.pem ca-key=/Users/codebase/.minikube/certs/ca.pem private-key=/Users/codebase/.minikube/certs/ca-key.pem org=eren.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0922 20:48:50.156758   69206 provision.go:177] copyRemoteCerts
I0922 20:48:50.157154   69206 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0922 20:48:50.157194   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:50.172012   69206 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55012 SSHKeyPath:/Users/codebase/.minikube/machines/minikube/id_rsa Username:docker}
I0922 20:48:50.270513   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0922 20:48:50.289698   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0922 20:48:50.301703   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0922 20:48:50.311518   69206 provision.go:87] duration metric: took 336.060917ms to configureAuth
I0922 20:48:50.311534   69206 ubuntu.go:206] setting minikube options for container-runtime
I0922 20:48:50.311655   69206 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0922 20:48:50.311739   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:50.326723   69206 main.go:141] libmachine: Using SSH client type: native
I0922 20:48:50.326899   69206 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c37c0] 0x1050c5f80 <nil>  [] 0s} 127.0.0.1 55012 <nil> <nil>}
I0922 20:48:50.326903   69206 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0922 20:48:50.452109   69206 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0922 20:48:50.452397   69206 ubuntu.go:71] root file system type: overlay
I0922 20:48:50.452523   69206 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0922 20:48:50.452698   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:50.479624   69206 main.go:141] libmachine: Using SSH client type: native
I0922 20:48:50.479874   69206 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c37c0] 0x1050c5f80 <nil>  [] 0s} 127.0.0.1 55012 <nil> <nil>}
I0922 20:48:50.479919   69206 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0922 20:48:50.611860   69206 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0922 20:48:50.612021   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:50.631083   69206 main.go:141] libmachine: Using SSH client type: native
I0922 20:48:50.631324   69206 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c37c0] 0x1050c5f80 <nil>  [] 0s} 127.0.0.1 55012 <nil> <nil>}
I0922 20:48:50.631335   69206 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0922 20:48:50.763880   69206 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0922 20:48:50.763902   69206 machine.go:96] duration metric: took 4.307214583s to provisionDockerMachine
I0922 20:48:50.763920   69206 start.go:293] postStartSetup for "minikube" (driver="docker")
I0922 20:48:50.763935   69206 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0922 20:48:50.764115   69206 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0922 20:48:50.764178   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:50.791594   69206 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55012 SSHKeyPath:/Users/codebase/.minikube/machines/minikube/id_rsa Username:docker}
I0922 20:48:50.887467   69206 ssh_runner.go:195] Run: cat /etc/os-release
I0922 20:48:50.890219   69206 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0922 20:48:50.890294   69206 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0922 20:48:50.890318   69206 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0922 20:48:50.890329   69206 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0922 20:48:50.890341   69206 filesync.go:126] Scanning /Users/codebase/.minikube/addons for local assets ...
I0922 20:48:50.890539   69206 filesync.go:126] Scanning /Users/codebase/.minikube/files for local assets ...
I0922 20:48:50.890626   69206 start.go:296] duration metric: took 126.7ms for postStartSetup
I0922 20:48:50.890756   69206 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0922 20:48:50.890861   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:50.921064   69206 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55012 SSHKeyPath:/Users/codebase/.minikube/machines/minikube/id_rsa Username:docker}
I0922 20:48:51.011507   69206 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0922 20:48:51.014883   69206 fix.go:56] duration metric: took 4.739109417s for fixHost
I0922 20:48:51.014901   69206 start.go:83] releasing machines lock for "minikube", held for 4.739644083s
I0922 20:48:51.015010   69206 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0922 20:48:51.046396   69206 ssh_runner.go:195] Run: cat /version.json
I0922 20:48:51.046506   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:51.046962   69206 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0922 20:48:51.047537   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:51.066582   69206 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55012 SSHKeyPath:/Users/codebase/.minikube/machines/minikube/id_rsa Username:docker}
I0922 20:48:51.066661   69206 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55012 SSHKeyPath:/Users/codebase/.minikube/machines/minikube/id_rsa Username:docker}
I0922 20:48:51.157161   69206 ssh_runner.go:195] Run: systemctl --version
I0922 20:48:51.379787   69206 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0922 20:48:51.383400   69206 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0922 20:48:51.395484   69206 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0922 20:48:51.395584   69206 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0922 20:48:51.400020   69206 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0922 20:48:51.400031   69206 start.go:495] detecting cgroup driver to use...
I0922 20:48:51.400050   69206 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0922 20:48:51.400873   69206 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0922 20:48:51.407973   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0922 20:48:51.411988   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0922 20:48:51.415752   69206 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0922 20:48:51.415800   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0922 20:48:51.419534   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0922 20:48:51.423116   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0922 20:48:51.426606   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0922 20:48:51.429860   69206 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0922 20:48:51.433097   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0922 20:48:51.436172   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0922 20:48:51.439300   69206 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0922 20:48:51.442497   69206 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0922 20:48:51.445718   69206 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0922 20:48:51.448482   69206 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0922 20:48:51.473475   69206 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0922 20:48:51.517898   69206 start.go:495] detecting cgroup driver to use...
I0922 20:48:51.517913   69206 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0922 20:48:51.518025   69206 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0922 20:48:51.522027   69206 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0922 20:48:51.525707   69206 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0922 20:48:51.533929   69206 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0922 20:48:51.537312   69206 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0922 20:48:51.541369   69206 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0922 20:48:51.545968   69206 ssh_runner.go:195] Run: which cri-dockerd
I0922 20:48:51.547248   69206 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0922 20:48:51.549687   69206 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0922 20:48:51.554784   69206 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0922 20:48:51.578445   69206 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0922 20:48:51.600794   69206 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I0922 20:48:51.600902   69206 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0922 20:48:51.606096   69206 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0922 20:48:51.609460   69206 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0922 20:48:51.632680   69206 ssh_runner.go:195] Run: sudo systemctl restart docker
I0922 20:48:52.616942   69206 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0922 20:48:52.621734   69206 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0922 20:48:52.625920   69206 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0922 20:48:52.630267   69206 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0922 20:48:52.634176   69206 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0922 20:48:52.659566   69206 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0922 20:48:52.682939   69206 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0922 20:48:52.704953   69206 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0922 20:48:52.734200   69206 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0922 20:48:52.737821   69206 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0922 20:48:52.760572   69206 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0922 20:48:52.889283   69206 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0922 20:48:52.893195   69206 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0922 20:48:52.893446   69206 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0922 20:48:52.894712   69206 start.go:563] Will wait 60s for crictl version
I0922 20:48:52.894767   69206 ssh_runner.go:195] Run: which crictl
I0922 20:48:52.895869   69206 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0922 20:48:52.950979   69206 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I0922 20:48:52.951061   69206 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0922 20:48:52.997584   69206 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0922 20:48:53.015332   69206 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I0922 20:48:53.015513   69206 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0922 20:48:53.102568   69206 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0922 20:48:53.102780   69206 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0922 20:48:53.104289   69206 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0922 20:48:53.107894   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0922 20:48:53.127021   69206 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0922 20:48:53.127083   69206 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0922 20:48:53.127131   69206 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0922 20:48:53.134008   69206 docker.go:691] Got preloaded images: -- stdout --
kubernetes-demo-api:latest
analystninja/k8s-demo:latest
jsmasterypro/kubernetes-demo-api:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0922 20:48:53.134012   69206 docker.go:621] Images already preloaded, skipping extraction
I0922 20:48:53.134219   69206 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0922 20:48:53.141432   69206 docker.go:691] Got preloaded images: -- stdout --
kubernetes-demo-api:latest
analystninja/k8s-demo:latest
jsmasterypro/kubernetes-demo-api:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0922 20:48:53.141439   69206 cache_images.go:85] Images are preloaded, skipping loading
I0922 20:48:53.141457   69206 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I0922 20:48:53.141528   69206 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0922 20:48:53.141572   69206 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0922 20:48:53.236378   69206 cni.go:84] Creating CNI manager for ""
I0922 20:48:53.236397   69206 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0922 20:48:53.236405   69206 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0922 20:48:53.236426   69206 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0922 20:48:53.236632   69206 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0922 20:48:53.236885   69206 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0922 20:48:53.240304   69206 binaries.go:44] Found k8s binaries, skipping transfer
I0922 20:48:53.240429   69206 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0922 20:48:53.243109   69206 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0922 20:48:53.248073   69206 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0922 20:48:53.252797   69206 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I0922 20:48:53.257988   69206 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0922 20:48:53.259309   69206 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0922 20:48:53.264090   69206 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0922 20:48:53.286696   69206 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0922 20:48:53.307827   69206 certs.go:68] Setting up /Users/codebase/.minikube/profiles/minikube for IP: 192.168.49.2
I0922 20:48:53.307832   69206 certs.go:194] generating shared ca certs ...
I0922 20:48:53.307840   69206 certs.go:226] acquiring lock for ca certs: {Name:mkdbe4409b5af22a2ca1c3a75633dce5fed0ba86 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0922 20:48:53.308302   69206 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/codebase/.minikube/ca.key
I0922 20:48:53.308647   69206 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/codebase/.minikube/proxy-client-ca.key
I0922 20:48:53.308658   69206 certs.go:256] generating profile certs ...
I0922 20:48:53.309082   69206 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/codebase/.minikube/profiles/minikube/client.key
I0922 20:48:53.309427   69206 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/codebase/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0922 20:48:53.309877   69206 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/codebase/.minikube/profiles/minikube/proxy-client.key
I0922 20:48:53.310628   69206 certs.go:484] found cert: /Users/codebase/.minikube/certs/ca-key.pem (1675 bytes)
I0922 20:48:53.310722   69206 certs.go:484] found cert: /Users/codebase/.minikube/certs/ca.pem (1074 bytes)
I0922 20:48:53.310772   69206 certs.go:484] found cert: /Users/codebase/.minikube/certs/cert.pem (1115 bytes)
I0922 20:48:53.310811   69206 certs.go:484] found cert: /Users/codebase/.minikube/certs/key.pem (1679 bytes)
I0922 20:48:53.311623   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0922 20:48:53.319161   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0922 20:48:53.326418   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0922 20:48:53.333494   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0922 20:48:53.340577   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0922 20:48:53.347834   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0922 20:48:53.354799   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0922 20:48:53.361794   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0922 20:48:53.368942   69206 ssh_runner.go:362] scp /Users/codebase/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0922 20:48:53.376379   69206 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0922 20:48:53.382341   69206 ssh_runner.go:195] Run: openssl version
I0922 20:48:53.386580   69206 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0922 20:48:53.390035   69206 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0922 20:48:53.391149   69206 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 21 07:52 /usr/share/ca-certificates/minikubeCA.pem
I0922 20:48:53.391226   69206 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0922 20:48:53.393651   69206 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0922 20:48:53.396467   69206 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0922 20:48:53.397746   69206 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0922 20:48:53.400917   69206 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0922 20:48:53.403374   69206 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0922 20:48:53.405779   69206 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0922 20:48:53.408088   69206 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0922 20:48:53.410345   69206 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0922 20:48:53.412710   69206 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0922 20:48:53.412800   69206 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0922 20:48:53.420622   69206 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0922 20:48:53.423696   69206 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0922 20:48:53.423884   69206 kubeadm.go:589] restartPrimaryControlPlane start ...
I0922 20:48:53.423979   69206 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0922 20:48:53.427537   69206 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0922 20:48:53.427644   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0922 20:48:53.450881   69206 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /Users/codebase/.kube/config
I0922 20:48:53.450966   69206 kubeconfig.go:62] /Users/codebase/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0922 20:48:53.451088   69206 lock.go:35] WriteFile acquiring /Users/codebase/.kube/config: {Name:mkb243309fbcdcfe6ca7e6c4eadae9b1a956d2ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0922 20:48:53.457978   69206 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0922 20:48:53.461737   69206 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0922 20:48:53.461749   69206 kubeadm.go:593] duration metric: took 37.862666ms to restartPrimaryControlPlane
I0922 20:48:53.461752   69206 kubeadm.go:394] duration metric: took 49.048ms to StartCluster
I0922 20:48:53.461761   69206 settings.go:142] acquiring lock: {Name:mkf9a2a4f2e0ced574943a44bc72821337771172 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0922 20:48:53.461833   69206 settings.go:150] Updating kubeconfig:  /Users/codebase/.kube/config
I0922 20:48:53.462097   69206 lock.go:35] WriteFile acquiring /Users/codebase/.kube/config: {Name:mkb243309fbcdcfe6ca7e6c4eadae9b1a956d2ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0922 20:48:53.462442   69206 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0922 20:48:53.462812   69206 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0922 20:48:53.462842   69206 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0922 20:48:53.462849   69206 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0922 20:48:53.462853   69206 addons.go:247] addon storage-provisioner should already be in state true
I0922 20:48:53.462850   69206 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0922 20:48:53.462862   69206 host.go:66] Checking if "minikube" exists ...
I0922 20:48:53.462871   69206 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0922 20:48:53.462869   69206 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0922 20:48:53.463221   69206 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0922 20:48:53.463235   69206 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0922 20:48:53.474524   69206 out.go:179] üîé  Verifying Kubernetes components...
I0922 20:48:53.477251   69206 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0922 20:48:53.480230   69206 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0922 20:48:53.480237   69206 addons.go:247] addon default-storageclass should already be in state true
I0922 20:48:53.480247   69206 host.go:66] Checking if "minikube" exists ...
I0922 20:48:53.480439   69206 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0922 20:48:53.484249   69206 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0922 20:48:53.488099   69206 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0922 20:48:53.488105   69206 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0922 20:48:53.488182   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:53.497732   69206 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0922 20:48:53.497750   69206 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0922 20:48:53.497862   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0922 20:48:53.502480   69206 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55012 SSHKeyPath:/Users/codebase/.minikube/machines/minikube/id_rsa Username:docker}
I0922 20:48:53.509212   69206 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0922 20:48:53.510098   69206 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55012 SSHKeyPath:/Users/codebase/.minikube/machines/minikube/id_rsa Username:docker}
I0922 20:48:53.513536   69206 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0922 20:48:53.524677   69206 api_server.go:52] waiting for apiserver process to appear ...
I0922 20:48:53.524803   69206 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0922 20:48:53.587823   69206 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0922 20:48:53.591325   69206 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0922 20:48:53.673755   69206 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0922 20:48:53.673780   69206 retry.go:31] will retry after 153.410408ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0922 20:48:53.673796   69206 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0922 20:48:53.673801   69206 retry.go:31] will retry after 248.970818ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0922 20:48:53.828646   69206 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0922 20:48:53.852039   69206 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0922 20:48:53.852053   69206 retry.go:31] will retry after 430.742367ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0922 20:48:53.923634   69206 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0922 20:48:53.949810   69206 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0922 20:48:53.949823   69206 retry.go:31] will retry after 494.118027ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0922 20:48:54.025753   69206 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0922 20:48:54.283580   69206 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0922 20:48:54.444811   69206 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0922 20:48:54.525596   69206 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0922 20:48:55.751060   69206 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.467461709s)
I0922 20:48:55.751095   69206 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.306279083s)
I0922 20:48:55.751124   69206 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.225524709s)
I0922 20:48:55.751134   69206 api_server.go:72] duration metric: took 2.288269209s to wait for apiserver process to appear ...
I0922 20:48:55.751144   69206 api_server.go:88] waiting for apiserver healthz status ...
I0922 20:48:55.751167   69206 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55011/healthz ...
I0922 20:48:55.758244   69206 api_server.go:279] https://127.0.0.1:55011/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0922 20:48:55.758260   69206 api_server.go:103] status: https://127.0.0.1:55011/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0922 20:48:55.770948   69206 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I0922 20:48:55.774952   69206 addons.go:514] duration metric: took 2.312303208s for enable addons: enabled=[storage-provisioner default-storageclass]
I0922 20:48:56.251701   69206 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55011/healthz ...
I0922 20:48:56.259030   69206 api_server.go:279] https://127.0.0.1:55011/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0922 20:48:56.259059   69206 api_server.go:103] status: https://127.0.0.1:55011/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0922 20:48:56.752280   69206 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55011/healthz ...
I0922 20:48:56.759752   69206 api_server.go:279] https://127.0.0.1:55011/healthz returned 200:
ok
I0922 20:48:56.761343   69206 api_server.go:141] control plane version: v1.34.0
I0922 20:48:56.761370   69206 api_server.go:131] duration metric: took 1.0102155s to wait for apiserver health ...
I0922 20:48:56.761589   69206 system_pods.go:43] waiting for kube-system pods to appear ...
I0922 20:48:56.769889   69206 system_pods.go:59] 5 kube-system pods found
I0922 20:48:56.769913   69206 system_pods.go:61] "etcd-minikube" [228d1d02-3a07-40e1-8b7b-93ed4ca1d9ad] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0922 20:48:56.769921   69206 system_pods.go:61] "kube-apiserver-minikube" [96867ad5-f807-453f-99c6-ec84625d86e3] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0922 20:48:56.769926   69206 system_pods.go:61] "kube-controller-manager-minikube" [e2170af2-96e5-4e1b-a640-6b798b1a44d9] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0922 20:48:56.769929   69206 system_pods.go:61] "kube-scheduler-minikube" [6ab8cbd3-acdb-4a4d-a41e-40e49c2fb106] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0922 20:48:56.769931   69206 system_pods.go:61] "storage-provisioner" [f37b63b2-3b88-4920-8694-7c1772b386a3] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0922 20:48:56.769935   69206 system_pods.go:74] duration metric: took 8.342625ms to wait for pod list to return data ...
I0922 20:48:56.769943   69206 kubeadm.go:578] duration metric: took 3.307085125s to wait for: map[apiserver:true system_pods:true]
I0922 20:48:56.769957   69206 node_conditions.go:102] verifying NodePressure condition ...
I0922 20:48:56.773420   69206 node_conditions.go:122] node storage ephemeral capacity is 474095688Ki
I0922 20:48:56.773437   69206 node_conditions.go:123] node cpu capacity is 10
I0922 20:48:56.773450   69206 node_conditions.go:105] duration metric: took 3.490334ms to run NodePressure ...
I0922 20:48:56.773458   69206 start.go:241] waiting for startup goroutines ...
I0922 20:48:56.773463   69206 start.go:246] waiting for cluster config update ...
I0922 20:48:56.773469   69206 start.go:255] writing updated cluster config ...
I0922 20:48:56.773976   69206 ssh_runner.go:195] Run: rm -f paused
I0922 20:48:56.935384   69206 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I0922 20:48:56.940386   69206 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 22 15:18:51 minikube dockerd[164]: time="2025-09-22T15:18:51.528686920Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Sep 22 15:18:51 minikube dockerd[164]: time="2025-09-22T15:18:51.528753462Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Sep 22 15:18:51 minikube systemd[1]: Stopping Docker Application Container Engine...
Sep 22 15:18:51 minikube dockerd[164]: time="2025-09-22T15:18:51.635729962Z" level=info msg="Processing signal 'terminated'"
Sep 22 15:18:51 minikube dockerd[164]: time="2025-09-22T15:18:51.636567920Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: grpc: the client connection is closing"
Sep 22 15:18:51 minikube dockerd[164]: time="2025-09-22T15:18:51.636669254Z" level=info msg="Daemon shutdown complete"
Sep 22 15:18:51 minikube systemd[1]: docker.service: Deactivated successfully.
Sep 22 15:18:51 minikube systemd[1]: Stopped Docker Application Container Engine.
Sep 22 15:18:51 minikube systemd[1]: Starting Docker Application Container Engine...
Sep 22 15:18:51 minikube dockerd[792]: time="2025-09-22T15:18:51.682669254Z" level=info msg="Starting up"
Sep 22 15:18:51 minikube dockerd[792]: time="2025-09-22T15:18:51.682989379Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Sep 22 15:18:51 minikube dockerd[792]: time="2025-09-22T15:18:51.683020504Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Sep 22 15:18:51 minikube dockerd[792]: time="2025-09-22T15:18:51.683028212Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Sep 22 15:18:51 minikube dockerd[792]: time="2025-09-22T15:18:51.686035295Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Sep 22 15:18:51 minikube dockerd[792]: time="2025-09-22T15:18:51.686777670Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Sep 22 15:18:51 minikube dockerd[792]: time="2025-09-22T15:18:51.691957212Z" level=info msg="Loading containers: start."
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.563668046Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count e5661a9962440925335e63f7544ba2edbaf3915fbb40ebc7b13099c376eaefa7], retrying...."
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.590189296Z" level=warning msg="error locating sandbox id 1b327a7ee3131b9d91c53f8540f4b2e42a8612399f0b9b08b0e2865b7783ce81: sandbox 1b327a7ee3131b9d91c53f8540f4b2e42a8612399f0b9b08b0e2865b7783ce81 not found"
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.590240837Z" level=warning msg="error locating sandbox id 03fa480909ab232f3efb2af20bbf3aa46abf339d0e0a089a3143a23546592156: sandbox 03fa480909ab232f3efb2af20bbf3aa46abf339d0e0a089a3143a23546592156 not found"
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.590420629Z" level=info msg="Loading containers: done."
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.599322879Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.599365962Z" level=info msg="Initializing buildkit"
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.613419004Z" level=info msg="Completed buildkit initialization"
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.615551796Z" level=info msg="Daemon has completed initialization"
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.615634296Z" level=info msg="API listen on /var/run/docker.sock"
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.615688296Z" level=info msg="API listen on /run/docker.sock"
Sep 22 15:18:52 minikube dockerd[792]: time="2025-09-22T15:18:52.615713171Z" level=info msg="API listen on [::]:2376"
Sep 22 15:18:52 minikube systemd[1]: Started Docker Application Container Engine.
Sep 22 15:18:52 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Start docker client with request timeout 0s"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Hairpin mode is set to hairpin-veth"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Loaded network plugin cni"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Docker cri networking managed by network plugin cni"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Setting cgroupDriver cgroupfs"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Sep 22 15:18:52 minikube cri-dockerd[1113]: time="2025-09-22T15:18:52Z" level=info msg="Start cri-dockerd grpc backend"
Sep 22 15:18:52 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"k8s-demo-64d4f88489-t6nvr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"129379d691c887e388b9b473ddf44019502e802220c572ef66b6a0e96b54b283\""
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"k8s-demo-64d4f88489-fstcd_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f57bc1b1ed6bd55f79abe4a9c49b4d8a0491a7a7b365194625d28d5f32e23744\""
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"2fca9548fd8f16cd0c8ed5ff83cd1dc146214666b654339872d93e54da59aabd\". Proceed without further sandbox information."
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"f6da98cb763ad3794a7fe56535c85cd86d0ea3300a3e0074a4e5f0824b9c1d33\". Proceed without further sandbox information."
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"a4d7a907f5b02637da2a23d585f06df7787d4640aed1dfa332ac7bcaa971bade\". Proceed without further sandbox information."
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"f75a6415f57e406154fca3e7f87fa885f6d3101bdaf848ef1c980a2c1306d4ed\". Proceed without further sandbox information."
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4ddc72f81d49ae919b393a19a4a3fd46105f93a247814dd40e13f8db7d472803/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09b5841235f17b859ce8949aaf39499ca7eefa3d02c7a9f107a6d865f3ca0f01/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ebc8f55d1d1b09f2bc6f88348c009d956453754ca2dfd2c633b172ee11971990/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 22 15:18:53 minikube cri-dockerd[1113]: time="2025-09-22T15:18:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6fa739d150302b76f843d9fb43582167ac383c9c9b3f82c1545e4647d1d440e7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 22 15:18:55 minikube cri-dockerd[1113]: time="2025-09-22T15:18:55Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Sep 22 15:18:56 minikube cri-dockerd[1113]: time="2025-09-22T15:18:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5be97b130a69d943c6129112a1d9fb14447bf7de43bffc09ba9aa5ffe78134c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 22 15:19:17 minikube cri-dockerd[1113]: time="2025-09-22T15:19:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b843a347da0fec0b9d4d9bd5ba54c318dc8ffd4f8fa4be7a3927b7fe1585803a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 22 15:19:17 minikube cri-dockerd[1113]: time="2025-09-22T15:19:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/72f7e3f8b9f5c5cec4867145e6f70830845d3be3566079a7d6e6747bf8629070/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 22 15:19:26 minikube dockerd[792]: time="2025-09-22T15:19:26.933592048Z" level=info msg="ignoring event" container=86c29023f6048096047c32f4a0dbedb0ed1f33eb4ed8d422def39f595b81ac4f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 22 15:19:28 minikube cri-dockerd[1113]: time="2025-09-22T15:19:28Z" level=info msg="Stop pulling image analystninja/kubernetes-demo-api:latest: Status: Downloaded newer image for analystninja/kubernetes-demo-api:latest"
Sep 22 15:19:30 minikube cri-dockerd[1113]: time="2025-09-22T15:19:30Z" level=info msg="Stop pulling image analystninja/kubernetes-demo-api:latest: Status: Image is up to date for analystninja/kubernetes-demo-api:latest"
Sep 22 15:20:13 minikube dockerd[792]: time="2025-09-22T15:20:13.549669166Z" level=info msg="ignoring event" container=074c6fc892206b4ae54ca88405e0d71240b942718cd9fef0efddc5ba59270ce9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 22 15:21:07 minikube dockerd[792]: time="2025-09-22T15:21:07.539942303Z" level=info msg="ignoring event" container=011d4df38f3f6f4c3810a53a8c1fb7f7708b6eb1e9e2053432e8e85f81710e4b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 22 15:22:30 minikube dockerd[792]: time="2025-09-22T15:22:30.539588341Z" level=info msg="ignoring event" container=3df45be617dbef2bafaa17b8b736373f6f02f008be9e40d3bd9a3ce1e3193d9d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                      CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
3df45be617dbe       ba04bb24b9575                                                                                              About a minute ago   Exited              storage-provisioner       19                  e5be97b130a69       storage-provisioner
79873a664a225       analystninja/kubernetes-demo-api@sha256:29926fea00addfbcb0363decbd5007feb5f33852c568afbcb9c11f915c046837   4 minutes ago        Running             kubernetes-demo-api       0                   72f7e3f8b9f5c       kubernetes-demo-api-55dc9f546d-l6lh2
305cee2784306       analystninja/kubernetes-demo-api@sha256:29926fea00addfbcb0363decbd5007feb5f33852c568afbcb9c11f915c046837   4 minutes ago        Running             kubernetes-demo-api       0                   b843a347da0fe       kubernetes-demo-api-55dc9f546d-6zqgs
e4bd3c7b2bde5       996be7e86d9b3                                                                                              4 minutes ago        Running             kube-controller-manager   5                   ebc8f55d1d1b0       kube-controller-manager-minikube
b1401a53baa2a       a25f5ef9c34c3                                                                                              4 minutes ago        Running             kube-scheduler            5                   6fa739d150302       kube-scheduler-minikube
7db69a9556fd9       d291939e99406                                                                                              4 minutes ago        Running             kube-apiserver            4                   4ddc72f81d49a       kube-apiserver-minikube
e55194cd7f307       a1894772a478e                                                                                              4 minutes ago        Running             etcd                      4                   09b5841235f17       etcd-minikube
320d73023578f       a1894772a478e                                                                                              27 hours ago         Exited              etcd                      3                   27c334aab0a9f       etcd-minikube
e7f5cf9be485d       a25f5ef9c34c3                                                                                              27 hours ago         Exited              kube-scheduler            4                   000fd967a4d20       kube-scheduler-minikube
3a21889d3e3d4       996be7e86d9b3                                                                                              27 hours ago         Exited              kube-controller-manager   4                   2498598cc643f       kube-controller-manager-minikube
db82b42d5e7ee       d291939e99406                                                                                              27 hours ago         Exited              kube-apiserver            3                   6d515a0a59d5f       kube-apiserver-minikube


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_09_21T13_22_46_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 21 Sep 2025 07:52:44 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 22 Sep 2025 15:23:21 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 22 Sep 2025 15:22:39 +0000   Sun, 21 Sep 2025 07:52:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 22 Sep 2025 15:22:39 +0000   Sun, 21 Sep 2025 07:52:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 22 Sep 2025 15:22:39 +0000   Sun, 21 Sep 2025 07:52:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 22 Sep 2025 15:22:39 +0000   Sun, 21 Sep 2025 07:52:44 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                10
  ephemeral-storage:  474095688Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025424Ki
  pods:               110
Allocatable:
  cpu:                10
  ephemeral-storage:  474095688Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025424Ki
  pods:               110
System Info:
  Machine ID:                 8e4d029266cb4c6a999e13e2b2122a13
  System UUID:                8e4d029266cb4c6a999e13e2b2122a13
  Boot ID:                    05920c12-8153-41ac-98d2-47f43c83c820
  Kernel Version:             6.10.14-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     kubernetes-demo-api-55dc9f546d-6zqgs    100m (1%)     500m (5%)   128Mi (1%)       512Mi (6%)     4m13s
  default                     kubernetes-demo-api-55dc9f546d-l6lh2    100m (1%)     500m (5%)   128Mi (1%)       512Mi (6%)     4m13s
  kube-system                 etcd-minikube                           100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         30h
  kube-system                 kube-apiserver-minikube                 250m (2%)     0 (0%)      0 (0%)           0 (0%)         30h
  kube-system                 kube-controller-manager-minikube        200m (2%)     0 (0%)      0 (0%)           0 (0%)         30h
  kube-system                 kube-scheduler-minikube                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         30h
  kube-system                 storage-provisioner                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         27h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (8%)   1 (10%)
  memory             356Mi (4%)  1Gi (13%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 31h                    kube-proxy       
  Normal   Starting                 31h                    kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  31h                    kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  31h                    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    31h                    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     31h                    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           31h                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeHasSufficientMemory  27h (x8 over 27h)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeAllocatableEnforced  27h                    kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID     27h (x7 over 27h)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure    27h (x8 over 27h)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   Starting                 27h                    kubelet          Starting kubelet.
  Warning  Rebooted                 27h                    kubelet          Node minikube has been rebooted, boot id: b6890411-f820-41a2-a88d-4d2a55d86bf6
  Normal   RegisteredNode           27h                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeHasSufficientPID     27h (x7 over 27h)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  27h                    kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 27h                    kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  27h (x8 over 27h)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    27h (x8 over 27h)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   RegisteredNode           27h                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeHasSufficientMemory  27h (x8 over 27h)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                 27h                    kubelet          Starting kubelet.
  Normal   NodeHasNoDiskPressure    27h (x8 over 27h)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     27h (x7 over 27h)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  27h                    kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           27h                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 4m37s                  kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  4m37s (x8 over 4m37s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    4m37s (x8 over 4m37s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     4m37s (x7 over 4m37s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  4m37s                  kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 4m35s                  kubelet          Node minikube has been rebooted, boot id: 05920c12-8153-41ac-98d2-47f43c83c820
  Normal   RegisteredNode           4m32s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Sep22 15:14] pci-host-generic 40000000.pci: Memory resource size exceeds max for 32 bits
[  +0.199668] netlink: 'init': attribute type 4 has an invalid length.
[  +0.157241] fakeowner: loading out-of-tree module taints kernel.


==> etcd [320d73023578] <==
{"level":"warn","ts":"2025-09-21T12:09:35.234074Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45948","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.240435Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45970","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.242867Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45988","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.245193Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:45992","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.247684Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46016","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.250045Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46038","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.252387Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46068","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.254837Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46086","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.257108Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46108","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.259498Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46128","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.263708Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46148","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.266071Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46170","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.269113Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46176","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.270738Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46206","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.273058Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46232","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.275581Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46244","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.278023Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46266","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.280400Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46284","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.283310Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46308","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.285670Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46328","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.287967Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46348","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.290321Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46372","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.292492Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46376","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.294821Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46392","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.297084Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46412","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.299447Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46432","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.316615Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46444","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.319017Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46468","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.321242Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46478","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.323576Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46490","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.325989Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46494","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.329213Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46504","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.330669Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46526","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.332967Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46542","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.335236Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46550","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.337332Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46582","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.339585Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46606","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.342228Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46636","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.362635Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46656","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.365013Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46674","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.367259Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46694","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-21T12:09:35.382451Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:46710","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-09-21T12:15:14.736326Z","caller":"osutil/interrupt_unix.go:65","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-09-21T12:15:14.736451Z","caller":"embed/etcd.go:426","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"error","ts":"2025-09-21T12:15:14.736595Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-09-21T12:15:21.740112Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-09-21T12:15:21.740272Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2381: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-09-21T12:15:21.740337Z","caller":"etcdserver/server.go:1281","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-09-21T12:15:21.740632Z","caller":"etcdserver/server.go:2319","msg":"server has stopped; stopping cluster version's monitor"}
{"level":"info","ts":"2025-09-21T12:15:21.740663Z","caller":"etcdserver/server.go:2342","msg":"server has stopped; stopping storage version's monitor"}
{"level":"warn","ts":"2025-09-21T12:15:21.742712Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-09-21T12:15:21.742772Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"error","ts":"2025-09-21T12:15:21.742789Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"warn","ts":"2025-09-21T12:15:21.742714Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-09-21T12:15:21.742802Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"error","ts":"2025-09-21T12:15:21.742807Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-09-21T12:15:21.745034Z","caller":"embed/etcd.go:621","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"error","ts":"2025-09-21T12:15:21.745126Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2380: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-09-21T12:15:21.745165Z","caller":"embed/etcd.go:626","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-09-21T12:15:21.745183Z","caller":"embed/etcd.go:428","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [e55194cd7f30] <==
{"level":"warn","ts":"2025-09-22T15:18:54.914427Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52758","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.919123Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52786","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.925323Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52804","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.928350Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52830","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.930910Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52848","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.934978Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52856","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.940299Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52876","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.943160Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52890","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.946202Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52914","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.949010Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52928","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:54.951564Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52956","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.003022Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52982","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.008174Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53000","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.010763Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53018","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.013120Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53036","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.015850Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53056","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.018179Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53066","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.021029Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53074","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.023523Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53100","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.035402Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53118","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.037182Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53128","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.039700Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53146","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.042136Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53152","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.044889Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53176","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.047238Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53198","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.050052Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53216","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.052614Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53228","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.104309Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53248","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.106939Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53272","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.109539Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53286","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.112059Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53300","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.114393Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53316","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.116979Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53336","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.119508Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53368","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.121948Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53380","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.124411Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53386","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.127841Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53408","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.131763Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53430","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.136004Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53446","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.138404Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53456","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.142504Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53484","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.144784Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53496","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.147163Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53522","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.150406Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53536","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.164329Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53550","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.166693Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53564","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.169134Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53576","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.171535Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53598","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.174015Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53616","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.176255Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53644","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.178798Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53664","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.181439Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53674","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.202459Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53684","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.204731Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53708","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.207021Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53732","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.222336Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53766","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.223892Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53780","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.226324Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53792","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.228636Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53812","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-09-22T15:18:55.246530Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:53822","server-name":"","error":"EOF"}


==> kernel <==
 15:23:30 up 9 min,  0 users,  load average: 3.40, 2.10, 0.91
Linux minikube 6.10.14-linuxkit #1 SMP Thu Aug 14 19:26:13 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [7db69a9556fd] <==
I0922 15:18:55.432808       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0922 15:18:55.432911       1 controller.go:119] Starting legacy_token_tracking_controller
I0922 15:18:55.432919       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I0922 15:18:55.433092       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I0922 15:18:55.433130       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0922 15:18:55.433327       1 repairip.go:210] Starting ipallocator-repair-controller
I0922 15:18:55.433337       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0922 15:18:55.433824       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0922 15:18:55.433831       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0922 15:18:55.433838       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0922 15:18:55.433930       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I0922 15:18:55.433934       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0922 15:18:55.434016       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0922 15:18:55.434020       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0922 15:18:55.435211       1 controller.go:142] Starting OpenAPI controller
I0922 15:18:55.435603       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0922 15:18:55.435665       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0922 15:18:55.435284       1 controller.go:90] Starting OpenAPI V3 controller
I0922 15:18:55.435297       1 naming_controller.go:299] Starting NamingConditionController
I0922 15:18:55.435309       1 establishing_controller.go:81] Starting EstablishingController
I0922 15:18:55.435323       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0922 15:18:55.435330       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0922 15:18:55.435336       1 crd_finalizer.go:269] Starting CRDFinalizer
I0922 15:18:55.501670       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I0922 15:18:55.509871       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0922 15:18:55.510298       1 policy_source.go:240] refreshing policies
I0922 15:18:55.525327       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0922 15:18:55.532409       1 cache.go:39] Caches are synced for LocalAvailability controller
I0922 15:18:55.532474       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0922 15:18:55.532478       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0922 15:18:55.533491       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I0922 15:18:55.533507       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I0922 15:18:55.533510       1 aggregator.go:171] initial CRD sync complete...
I0922 15:18:55.533516       1 autoregister_controller.go:144] Starting autoregister controller
I0922 15:18:55.533518       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0922 15:18:55.533521       1 cache.go:39] Caches are synced for autoregister controller
I0922 15:18:55.533577       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0922 15:18:55.533599       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I0922 15:18:55.533722       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I0922 15:18:55.534236       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0922 15:18:55.534242       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0922 15:18:55.534281       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I0922 15:18:55.605517       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0922 15:18:55.607477       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0922 15:18:55.609787       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0922 15:18:56.437208       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0922 15:18:56.465771       1 controller.go:667] quota admission added evaluator for: serviceaccounts
E0922 15:19:05.720893       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 192f4950-dc0a-4eef-9180-a81eca6c834a, UID in object meta: "
I0922 15:19:17.420400       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0922 15:19:17.425987       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0922 15:19:17.427433       1 alloc.go:328] "allocated clusterIPs" service="default/devops-kubernetes-api-service" clusterIPs={"IPv4":"10.107.98.60"}
I0922 15:19:17.432374       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0922 15:19:17.432696       1 controller.go:667] quota admission added evaluator for: endpoints
I0922 15:19:56.022101       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0922 15:20:12.378102       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0922 15:21:03.835486       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0922 15:21:39.279003       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0922 15:22:18.770645       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0922 15:23:03.223909       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0922 15:23:23.196075       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [db82b42d5e7e] <==
W0921 12:15:20.521951       1 logging.go:55] [core] [Channel #183 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:20.545001       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:20.550874       1 logging.go:55] [core] [Channel #231 SubChannel #233]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:20.615854       1 logging.go:55] [core] [Channel #211 SubChannel #213]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:21.185408       1 logging.go:55] [core] [Channel #283 SubChannel #284]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.012013       1 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.012013       1 logging.go:55] [core] [Channel #167 SubChannel #169]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.257400       1 logging.go:55] [core] [Channel #135 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.363631       1 logging.go:55] [core] [Channel #63 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.385098       1 logging.go:55] [core] [Channel #195 SubChannel #197]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.389002       1 logging.go:55] [core] [Channel #123 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.417396       1 logging.go:55] [core] [Channel #147 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.463409       1 logging.go:55] [core] [Channel #207 SubChannel #209]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.467671       1 logging.go:55] [core] [Channel #235 SubChannel #237]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.525992       1 logging.go:55] [core] [Channel #191 SubChannel #193]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.554379       1 logging.go:55] [core] [Channel #159 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.561095       1 logging.go:55] [core] [Channel #215 SubChannel #217]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.573406       1 logging.go:55] [core] [Channel #75 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.585110       1 logging.go:55] [core] [Channel #247 SubChannel #249]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.618182       1 logging.go:55] [core] [Channel #59 SubChannel #61]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.622615       1 logging.go:55] [core] [Channel #95 SubChannel #97]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.630071       1 logging.go:55] [core] [Channel #139 SubChannel #141]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.673989       1 logging.go:55] [core] [Channel #243 SubChannel #245]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.711703       1 logging.go:55] [core] [Channel #131 SubChannel #133]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.714143       1 logging.go:55] [core] [Channel #179 SubChannel #181]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.737453       1 logging.go:55] [core] [Channel #21 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.756757       1 logging.go:55] [core] [Channel #39 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.775251       1 logging.go:55] [core] [Channel #99 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.802130       1 logging.go:55] [core] [Channel #1 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.821756       1 logging.go:55] [core] [Channel #171 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.833199       1 logging.go:55] [core] [Channel #83 SubChannel #85]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.924690       1 logging.go:55] [core] [Channel #151 SubChannel #153]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.940484       1 logging.go:55] [core] [Channel #11 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.970078       1 logging.go:55] [core] [Channel #227 SubChannel #229]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.980275       1 logging.go:55] [core] [Channel #111 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.980363       1 logging.go:55] [core] [Channel #163 SubChannel #165]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:23.995333       1 logging.go:55] [core] [Channel #43 SubChannel #45]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.013174       1 logging.go:55] [core] [Channel #35 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.029233       1 logging.go:55] [core] [Channel #27 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.046206       1 logging.go:55] [core] [Channel #255 SubChannel #257]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.105187       1 logging.go:55] [core] [Channel #219 SubChannel #221]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.109638       1 logging.go:55] [core] [Channel #187 SubChannel #189]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.132134       1 logging.go:55] [core] [Channel #155 SubChannel #157]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.201312       1 logging.go:55] [core] [Channel #115 SubChannel #117]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.210133       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.241483       1 logging.go:55] [core] [Channel #107 SubChannel #109]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.279165       1 logging.go:55] [core] [Channel #199 SubChannel #201]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.321092       1 logging.go:55] [core] [Channel #203 SubChannel #205]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.402309       1 logging.go:55] [core] [Channel #79 SubChannel #81]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.449148       1 logging.go:55] [core] [Channel #251 SubChannel #253]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.460119       1 logging.go:55] [core] [Channel #175 SubChannel #177]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.484383       1 logging.go:55] [core] [Channel #55 SubChannel #57]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.544388       1 logging.go:55] [core] [Channel #231 SubChannel #233]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.570211       1 logging.go:55] [core] [Channel #239 SubChannel #241]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.598100       1 logging.go:55] [core] [Channel #7 SubChannel #9]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.598101       1 logging.go:55] [core] [Channel #183 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.598118       1 logging.go:55] [core] [Channel #143 SubChannel #145]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.603799       1 logging.go:55] [core] [Channel #47 SubChannel #49]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.614730       1 logging.go:55] [core] [Channel #87 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0921 12:15:24.653344       1 logging.go:55] [core] [Channel #71 SubChannel #73]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [3a21889d3e3d] <==
I0921 12:09:38.813530       1 shared_informer.go:349] "Waiting for caches to sync" controller="job"
I0921 12:09:38.862296       1 controllermanager.go:781] "Started controller" controller="deployment-controller"
I0921 12:09:38.864346       1 deployment_controller.go:173] "Starting controller" logger="deployment-controller" controller="deployment"
I0921 12:09:38.864389       1 shared_informer.go:349] "Waiting for caches to sync" controller="deployment"
I0921 12:09:38.867112       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0921 12:09:38.879941       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0921 12:09:38.880102       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0921 12:09:38.881606       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0921 12:09:38.883136       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0921 12:09:38.887495       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0921 12:09:38.887524       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0921 12:09:38.889321       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0921 12:09:38.891329       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0921 12:09:38.891354       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0921 12:09:38.898495       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0921 12:09:38.898544       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0921 12:09:38.898555       1 shared_informer.go:356] "Caches are synced" controller="node"
I0921 12:09:38.898571       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0921 12:09:38.898589       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0921 12:09:38.898592       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0921 12:09:38.898596       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0921 12:09:38.914302       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0921 12:09:38.914338       1 shared_informer.go:356] "Caches are synced" controller="job"
I0921 12:09:38.914343       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0921 12:09:38.914408       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0921 12:09:38.915194       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0921 12:09:38.915211       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0921 12:09:38.915216       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0921 12:09:38.915312       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0921 12:09:38.915372       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0921 12:09:38.915430       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0921 12:09:38.915465       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0921 12:09:38.918457       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0921 12:09:38.962762       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0921 12:09:38.962812       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0921 12:09:38.962774       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0921 12:09:38.963020       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0921 12:09:38.963106       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0921 12:09:38.963210       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0921 12:09:38.964337       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0921 12:09:38.964440       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0921 12:09:38.965270       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0921 12:09:38.965297       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0921 12:09:38.965300       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0921 12:09:38.967284       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0921 12:09:38.967341       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0921 12:09:38.967366       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0921 12:09:38.967378       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0921 12:09:38.967451       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0921 12:09:38.970229       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0921 12:09:38.971346       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0921 12:09:38.973514       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0921 12:09:38.974196       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0921 12:09:38.977320       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0921 12:09:38.977371       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0921 12:09:38.979679       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0921 12:09:38.981132       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0921 12:09:38.981224       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0921 12:09:38.983371       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0921 12:15:04.610189       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/devops-kubernetes-api-service" err="EndpointSlice informer cache is out of date"


==> kube-controller-manager [e4bd3c7b2bde] <==
I0922 15:18:58.800629       1 shared_informer.go:349] "Waiting for caches to sync" controller="daemon sets"
I0922 15:18:58.848982       1 controllermanager.go:781] "Started controller" controller="cronjob-controller"
I0922 15:18:58.849010       1 controllermanager.go:739] "Skipping a cloud provider controller" controller="cloud-node-lifecycle-controller"
I0922 15:18:58.850601       1 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
I0922 15:18:58.850627       1 shared_informer.go:349] "Waiting for caches to sync" controller="cronjob"
I0922 15:18:58.857175       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0922 15:18:58.867329       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0922 15:18:58.871166       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0922 15:18:58.877692       1 shared_informer.go:356] "Caches are synced" controller="node"
I0922 15:18:58.877717       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0922 15:18:58.877726       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0922 15:18:58.877740       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0922 15:18:58.877744       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0922 15:18:58.877747       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0922 15:18:58.877884       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0922 15:18:58.879477       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0922 15:18:58.881668       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0922 15:18:58.882442       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0922 15:18:58.885699       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0922 15:18:58.885724       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0922 15:18:58.885732       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0922 15:18:58.885913       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0922 15:18:58.887517       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0922 15:18:58.889476       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0922 15:18:58.889489       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0922 15:18:58.889496       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0922 15:18:58.890793       1 shared_informer.go:356] "Caches are synced" controller="job"
I0922 15:18:58.893513       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0922 15:18:58.895480       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0922 15:18:58.895504       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0922 15:18:58.896476       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0922 15:18:58.899697       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0922 15:18:58.899729       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0922 15:18:58.899752       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0922 15:18:58.899754       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0922 15:18:58.901524       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0922 15:18:58.901535       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0922 15:18:58.901545       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0922 15:18:58.901578       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0922 15:18:58.901589       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0922 15:18:58.906146       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0922 15:18:58.948738       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0922 15:18:58.950446       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0922 15:18:58.950476       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0922 15:18:58.950485       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0922 15:18:58.950427       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0922 15:18:58.950589       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0922 15:18:58.950692       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0922 15:18:58.950752       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0922 15:18:58.950850       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0922 15:18:58.951330       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0922 15:18:58.951355       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0922 15:18:58.952357       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0922 15:18:58.954457       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0922 15:18:58.956494       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0922 15:18:58.957833       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0922 15:18:58.959437       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0922 15:18:58.960434       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0922 15:18:58.961461       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0922 15:18:58.971294       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-scheduler [b1401a53baa2] <==
I0922 15:18:54.523008       1 serving.go:386] Generated self-signed cert in-memory
W0922 15:18:55.501545       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0922 15:18:55.501591       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0922 15:18:55.501603       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0922 15:18:55.501609       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0922 15:18:55.514278       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0922 15:18:55.514328       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0922 15:18:55.515283       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0922 15:18:55.515335       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0922 15:18:55.515488       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0922 15:18:55.515677       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0922 15:18:55.616271       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [e7f5cf9be485] <==
I0921 12:09:34.239439       1 serving.go:386] Generated self-signed cert in-memory
I0921 12:09:35.564570       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0921 12:09:35.564588       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0921 12:09:35.630720       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0921 12:09:35.630775       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0921 12:09:35.630782       1 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I0921 12:09:35.630794       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0921 12:09:35.631110       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0921 12:09:35.631119       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0921 12:09:35.631122       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0921 12:09:35.631147       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0921 12:09:35.731305       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0921 12:09:35.731327       1 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"
I0921 12:09:35.731368       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0921 12:15:14.735400       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0921 12:15:14.735438       1 requestheader_controller.go:194] Shutting down RequestHeaderAuthRequestController
I0921 12:15:14.735460       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0921 12:15:14.735419       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0921 12:15:14.735549       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I0921 12:15:14.735566       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I0921 12:15:14.735585       1 server.go:265] "[graceful-termination] secure server is exiting"
E0921 12:15:14.735620       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Sep 22 15:18:55 minikube kubelet[1343]: E0922 15:18:55.610646    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: E0922 15:18:55.610672    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: I0922 15:18:55.610683    1343 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: I0922 15:18:55.610752    1343 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Sep 22 15:18:55 minikube kubelet[1343]: E0922 15:18:55.610650    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: I0922 15:18:55.610798    1343 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Sep 22 15:18:55 minikube kubelet[1343]: I0922 15:18:55.610842    1343 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Sep 22 15:18:55 minikube kubelet[1343]: E0922 15:18:55.610781    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: I0922 15:18:55.611276    1343 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Sep 22 15:18:55 minikube kubelet[1343]: E0922 15:18:55.613514    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: I0922 15:18:55.613527    1343 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: E0922 15:18:55.615328    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: I0922 15:18:55.615339    1343 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Sep 22 15:18:55 minikube kubelet[1343]: E0922 15:18:55.617005    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: I0922 15:18:56.423905    1343 apiserver.go:52] "Watching apiserver"
Sep 22 15:18:56 minikube kubelet[1343]: I0922 15:18:56.429079    1343 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Sep 22 15:18:56 minikube kubelet[1343]: I0922 15:18:56.464429    1343 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/f37b63b2-3b88-4920-8694-7c1772b386a3-tmp\") pod \"storage-provisioner\" (UID: \"f37b63b2-3b88-4920-8694-7c1772b386a3\") " pod="kube-system/storage-provisioner"
Sep 22 15:18:56 minikube kubelet[1343]: I0922 15:18:56.531732    1343 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: I0922 15:18:56.531757    1343 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: I0922 15:18:56.531825    1343 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: I0922 15:18:56.531939    1343 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: E0922 15:18:56.534259    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: E0922 15:18:56.534320    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: E0922 15:18:56.534496    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Sep 22 15:18:56 minikube kubelet[1343]: E0922 15:18:56.534529    1343 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Sep 22 15:19:17 minikube kubelet[1343]: I0922 15:19:17.551397    1343 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-92zjd\" (UniqueName: \"kubernetes.io/projected/275127dd-4991-43cf-b3aa-ab0441f3fa92-kube-api-access-92zjd\") pod \"kubernetes-demo-api-55dc9f546d-6zqgs\" (UID: \"275127dd-4991-43cf-b3aa-ab0441f3fa92\") " pod="default/kubernetes-demo-api-55dc9f546d-6zqgs"
Sep 22 15:19:17 minikube kubelet[1343]: I0922 15:19:17.551457    1343 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dn5st\" (UniqueName: \"kubernetes.io/projected/e6a506b7-6176-495d-ada6-b25f5ecc9783-kube-api-access-dn5st\") pod \"kubernetes-demo-api-55dc9f546d-l6lh2\" (UID: \"e6a506b7-6176-495d-ada6-b25f5ecc9783\") " pod="default/kubernetes-demo-api-55dc9f546d-l6lh2"
Sep 22 15:19:27 minikube kubelet[1343]: I0922 15:19:27.806000    1343 scope.go:117] "RemoveContainer" containerID="38388ca0bb5bce3b9be9baa971501b280a4638b2963d792b475a9b82aeea0c0d"
Sep 22 15:19:27 minikube kubelet[1343]: I0922 15:19:27.806432    1343 scope.go:117] "RemoveContainer" containerID="86c29023f6048096047c32f4a0dbedb0ed1f33eb4ed8d422def39f595b81ac4f"
Sep 22 15:19:27 minikube kubelet[1343]: E0922 15:19:27.806601    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:19:28 minikube kubelet[1343]: I0922 15:19:28.822942    1343 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/kubernetes-demo-api-55dc9f546d-6zqgs" podStartSLOduration=1.651461752 podStartE2EDuration="11.822925799s" podCreationTimestamp="2025-09-22 15:19:17 +0000 UTC" firstStartedPulling="2025-09-22 15:19:17.86532696 +0000 UTC m=+24.536584845" lastFinishedPulling="2025-09-22 15:19:28.036791007 +0000 UTC m=+34.708048892" observedRunningTime="2025-09-22 15:19:28.822745215 +0000 UTC m=+35.494003100" watchObservedRunningTime="2025-09-22 15:19:28.822925799 +0000 UTC m=+35.494183684"
Sep 22 15:19:30 minikube kubelet[1343]: I0922 15:19:30.837003    1343 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/kubernetes-demo-api-55dc9f546d-l6lh2" podStartSLOduration=1.203935752 podStartE2EDuration="13.836994008s" podCreationTimestamp="2025-09-22 15:19:17 +0000 UTC" firstStartedPulling="2025-09-22 15:19:17.865423543 +0000 UTC m=+24.536681470" lastFinishedPulling="2025-09-22 15:19:30.498481799 +0000 UTC m=+37.169739726" observedRunningTime="2025-09-22 15:19:30.836772216 +0000 UTC m=+37.508030143" watchObservedRunningTime="2025-09-22 15:19:30.836994008 +0000 UTC m=+37.508251935"
Sep 22 15:19:43 minikube kubelet[1343]: I0922 15:19:43.435767    1343 scope.go:117] "RemoveContainer" containerID="86c29023f6048096047c32f4a0dbedb0ed1f33eb4ed8d422def39f595b81ac4f"
Sep 22 15:20:14 minikube kubelet[1343]: I0922 15:20:14.248430    1343 scope.go:117] "RemoveContainer" containerID="86c29023f6048096047c32f4a0dbedb0ed1f33eb4ed8d422def39f595b81ac4f"
Sep 22 15:20:14 minikube kubelet[1343]: I0922 15:20:14.248827    1343 scope.go:117] "RemoveContainer" containerID="074c6fc892206b4ae54ca88405e0d71240b942718cd9fef0efddc5ba59270ce9"
Sep 22 15:20:14 minikube kubelet[1343]: E0922 15:20:14.249016    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:20:24 minikube kubelet[1343]: I0922 15:20:24.436255    1343 scope.go:117] "RemoveContainer" containerID="074c6fc892206b4ae54ca88405e0d71240b942718cd9fef0efddc5ba59270ce9"
Sep 22 15:20:24 minikube kubelet[1343]: E0922 15:20:24.436532    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:20:37 minikube kubelet[1343]: I0922 15:20:37.435096    1343 scope.go:117] "RemoveContainer" containerID="074c6fc892206b4ae54ca88405e0d71240b942718cd9fef0efddc5ba59270ce9"
Sep 22 15:21:07 minikube kubelet[1343]: I0922 15:21:07.716471    1343 scope.go:117] "RemoveContainer" containerID="074c6fc892206b4ae54ca88405e0d71240b942718cd9fef0efddc5ba59270ce9"
Sep 22 15:21:07 minikube kubelet[1343]: I0922 15:21:07.716628    1343 scope.go:117] "RemoveContainer" containerID="011d4df38f3f6f4c3810a53a8c1fb7f7708b6eb1e9e2053432e8e85f81710e4b"
Sep 22 15:21:07 minikube kubelet[1343]: E0922 15:21:07.716728    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:21:21 minikube kubelet[1343]: I0922 15:21:21.437390    1343 scope.go:117] "RemoveContainer" containerID="011d4df38f3f6f4c3810a53a8c1fb7f7708b6eb1e9e2053432e8e85f81710e4b"
Sep 22 15:21:21 minikube kubelet[1343]: E0922 15:21:21.437689    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:21:32 minikube kubelet[1343]: I0922 15:21:32.435697    1343 scope.go:117] "RemoveContainer" containerID="011d4df38f3f6f4c3810a53a8c1fb7f7708b6eb1e9e2053432e8e85f81710e4b"
Sep 22 15:21:32 minikube kubelet[1343]: E0922 15:21:32.436014    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:21:46 minikube kubelet[1343]: I0922 15:21:46.433703    1343 scope.go:117] "RemoveContainer" containerID="011d4df38f3f6f4c3810a53a8c1fb7f7708b6eb1e9e2053432e8e85f81710e4b"
Sep 22 15:21:46 minikube kubelet[1343]: E0922 15:21:46.433774    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:22:00 minikube kubelet[1343]: I0922 15:22:00.435496    1343 scope.go:117] "RemoveContainer" containerID="011d4df38f3f6f4c3810a53a8c1fb7f7708b6eb1e9e2053432e8e85f81710e4b"
Sep 22 15:22:31 minikube kubelet[1343]: I0922 15:22:31.452430    1343 scope.go:117] "RemoveContainer" containerID="011d4df38f3f6f4c3810a53a8c1fb7f7708b6eb1e9e2053432e8e85f81710e4b"
Sep 22 15:22:31 minikube kubelet[1343]: I0922 15:22:31.453019    1343 scope.go:117] "RemoveContainer" containerID="3df45be617dbef2bafaa17b8b736373f6f02f008be9e40d3bd9a3ce1e3193d9d"
Sep 22 15:22:31 minikube kubelet[1343]: E0922 15:22:31.453198    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:22:44 minikube kubelet[1343]: I0922 15:22:44.434603    1343 scope.go:117] "RemoveContainer" containerID="3df45be617dbef2bafaa17b8b736373f6f02f008be9e40d3bd9a3ce1e3193d9d"
Sep 22 15:22:44 minikube kubelet[1343]: E0922 15:22:44.434871    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:22:59 minikube kubelet[1343]: I0922 15:22:59.435129    1343 scope.go:117] "RemoveContainer" containerID="3df45be617dbef2bafaa17b8b736373f6f02f008be9e40d3bd9a3ce1e3193d9d"
Sep 22 15:22:59 minikube kubelet[1343]: E0922 15:22:59.435447    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:23:11 minikube kubelet[1343]: I0922 15:23:11.437132    1343 scope.go:117] "RemoveContainer" containerID="3df45be617dbef2bafaa17b8b736373f6f02f008be9e40d3bd9a3ce1e3193d9d"
Sep 22 15:23:11 minikube kubelet[1343]: E0922 15:23:11.437430    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"
Sep 22 15:23:25 minikube kubelet[1343]: I0922 15:23:25.436209    1343 scope.go:117] "RemoveContainer" containerID="3df45be617dbef2bafaa17b8b736373f6f02f008be9e40d3bd9a3ce1e3193d9d"
Sep 22 15:23:25 minikube kubelet[1343]: E0922 15:23:25.436637    1343 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(f37b63b2-3b88-4920-8694-7c1772b386a3)\"" pod="kube-system/storage-provisioner" podUID="f37b63b2-3b88-4920-8694-7c1772b386a3"


==> storage-provisioner [3df45be617db] <==
I0922 15:22:00.521949       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0922 15:22:30.524672       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

